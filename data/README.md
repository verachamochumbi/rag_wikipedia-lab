# üìä Data Directory

This directory stores the Wikipedia corpus used for the RAG system.

## üìÑ Files

### `wiki_corpus.csv`
**Purpose**: Structured storage of scraped Wikipedia articles

**Schema**:
| Column | Type | Description | Example |
|--------|------|-------------|---------|
| `id` | Integer | Unique article identifier | 1 |
| `title` | String | Wikipedia article title | "Federated_Learning" |
| `text` | String | Full article content | "Federated learning (also known as..." |

**Sample Structure**:
```csv
id,title,text
1,Federated_Learning,"Federated learning (also known as collaborative learning) is a machine learning technique..."
```

## üìè Current Corpus Statistics

**Articles**: 1  
**Topic**: Federated Learning  
**Source**: Wikipedia (English)  
**Total Words**: ~4,500  
**Total Characters**: ~30,000  
**Sections**: 15+ (Introduction, History, Methods, Applications, etc.)

**Chunks Generated**: 15 (300 words each)

## üîÑ Data Collection Process

The corpus is generated by the notebook (`notebooks/rag_wikipedia.ipynb`):

### Step 1: Fetch Article (Cell 1)
```python
import wikipediaapi

wiki = wikipediaapi.Wikipedia(
    language='en',
    user_agent='MyStudentProject (https://github.com/tu-usuario)'
)

page = wiki.page("Federated_learning")
text = page.text
```

### Step 2: Structure Data (Cell 3)
```python
import pandas as pd

df = pd.DataFrame([{
    "id": 1,
    "title": "Federated_Learning",
    "text": text
}])

df.to_csv("data/wiki_corpus.csv", index=False)
```

### Step 3: Verify (Cell 4)
```python
pd.read_csv("data/wiki_corpus.csv").head()
```

## üìö Expanding the Corpus

### Adding More Articles

To include additional Wikipedia topics, modify the notebook:

```python
topics = ["Federated_learning", "Machine_learning", "Artificial_intelligence"]
articles = []

for topic in topics:
    page = wiki.page(topic)
    if page.exists():
        articles.append({
            "id": len(articles) + 1,
            "title": topic,
            "text": page.text
        })

df = pd.DataFrame(articles)
df.to_csv("data/wiki_corpus.csv", index=False)
```

### Recommended Topics for Expansion

**AI & Machine Learning**:
- Neural_network
- Deep_learning
- Natural_language_processing
- Computer_vision

**Privacy & Security**:
- Differential_privacy
- Homomorphic_encryption
- Secure_multi-party_computation

**Healthcare AI**:
- Medical_imaging
- Electronic_health_record
- Clinical_decision_support_system

## üßπ Data Quality Considerations

### Preprocessing Applied
‚úÖ Raw Wikipedia text (no HTML)  
‚úÖ UTF-8 encoding  
‚úÖ Preserved original formatting (sections, lists)  
‚ùå No cleaning or filtering (keeps tables, references, etc.)

### Known Issues
‚ö†Ô∏è Contains Wikipedia markup artifacts (brackets, reference numbers)  
‚ö†Ô∏è May include "See also" and "References" sections  
‚ö†Ô∏è Lists are flattened to text format

### Potential Improvements
- [ ] Remove citation brackets ([1], [2], etc.)
- [ ] Filter out "References" and "External links" sections
- [ ] Convert tables to structured format
- [ ] Add article metadata (date, categories, infobox)
- [ ] Implement incremental updates

## üìä Data Format Options

### Alternative Formats

**JSON (for nested structure)**:
```json
{
  "articles": [
    {
      "id": 1,
      "title": "Federated_Learning",
      "text": "...",
      "metadata": {
        "url": "https://en.wikipedia.org/wiki/Federated_learning",
        "last_modified": "2024-10-15",
        "categories": ["Machine learning", "Privacy"]
      }
    }
  ]
}
```

**SQLite (for scalability)**:
```sql
CREATE TABLE articles (
    id INTEGER PRIMARY KEY,
    title TEXT NOT NULL,
    text TEXT NOT NULL,
    url TEXT,
    scraped_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
```

**Parquet (for big data)**:
```python
df.to_parquet("data/wiki_corpus.parquet", compression="snappy")
```

## üîê Data Usage & Licensing

### Wikipedia Content License
All content is from **Wikipedia** and licensed under:
- [CC BY-SA 3.0](https://creativecommons.org/licenses/by-sa/3.0/)
- [GFDL](https://www.gnu.org/licenses/fdl-1.3.html)

### Attribution Requirements
When using this data:
‚úÖ Mention Wikipedia as the source  
‚úÖ Link to original articles  
‚úÖ Share derivatives under the same license  
‚úÖ Respect the user_agent policy

### User Agent Policy
```python
# Good: Descriptive user agent
user_agent='MyProject/1.0 (contact@example.com)'

# Bad: Generic or missing user agent
user_agent='Python'
```

## üìà Storage Considerations

### Current Size
- **CSV File**: ~35 KB (1 article)
- **After 10 articles**: ~350 KB
- **After 100 articles**: ~3.5 MB
- **After 1000 articles**: ~35 MB

### Scalability
For large corpora (1000+ articles):
- Consider Parquet format (30-50% size reduction)
- Implement database storage (SQLite/PostgreSQL)
- Use chunked reading for processing
- Add data versioning (DVC)

## üîç Querying the Data

### Load into Pandas
```python
import pandas as pd
df = pd.read_csv("data/wiki_corpus.csv")

# Get specific article
article = df[df['title'] == 'Federated_Learning']['text'].values[0]

# Get all titles
titles = df['title'].tolist()

# Count total words
total_words = df['text'].str.split().str.len().sum()
```

### Search for Keywords
```python
# Find articles mentioning "privacy"
privacy_articles = df[df['text'].str.contains('privacy', case=False)]

# Count keyword occurrences
keyword_counts = df['text'].str.count('machine learning')
```

## üöÄ Next Steps

### Data Enhancement
1. Add more articles on related topics
2. Include article metadata (date, categories)
3. Extract and structure infoboxes
4. Link related articles (graph structure)

### Data Validation
1. Check for duplicate articles
2. Verify text encoding
3. Ensure all articles have content
4. Log any scraping failures

### Automation
```python
# Scheduled updates
from datetime import datetime

def update_corpus():
    """Refresh articles weekly"""
    for title in df['title']:
        page = wiki.page(title)
        # Update if changed
```

## üîó Related Documentation

- Main README: `../README.md`
- Notebook documentation: `../notebooks/README.md`
- Outputs documentation: `../outputs/README.md`

---

**Data Source**: [Wikipedia](https://www.wikipedia.org/)  
**Last Updated**: November 2025  
**Format Version**: 1.0

